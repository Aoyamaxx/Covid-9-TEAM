{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtering google trurh values\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df_fmain = pd.read_csv('raw_datasets/supplementary_dataset_TS/epidemiology.csv')\n",
    "\n",
    "# Define the regex pattern for US states\n",
    "pattern = r'^US_[A-Z]{2}$'\n",
    "\n",
    "# Create a mask, replacing NaN values with False\n",
    "mask = df_fmain['location_key'].str.match(pattern).fillna(False)\n",
    "\n",
    "# Apply the mask to filter rows\n",
    "df_filtered = df_fmain[mask]\n",
    "\n",
    "# Continue with your intended groupby and aggregation operation\n",
    "# (Ensure 'date' and 'new_confirmed' columns exist in your DataFrame)\n",
    "grouped_df_fmain = df_filtered.groupby(['date', 'location_key']).agg({\n",
    "    'new_confirmed': 'first',  # Adjust the aggregation as necessary\n",
    "}).reset_index()\n",
    "\n",
    "# Save the grouped DataFrame\n",
    "grouped_df_fmain.to_csv('processed_data/unique_truth_google_dataset.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time seris correlation ranking main dataset\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Adjust these paths to where your datasets are located\n",
    "truth_data_path = 'processed_data/unique_truth_main_dataset.csv'\n",
    "datasets_folder_path = 'raw_datasets/supplementary_dataset_TS'\n",
    "\n",
    "# Load the truth data\n",
    "truth_data = pd.read_csv(truth_data_path)\n",
    "truth_data.rename(columns={'abbreviation': 'location_key', 'target_end_date': 'date'}, inplace=True)\n",
    "truth_data['location_key'] = 'US_' + truth_data['location_key']\n",
    "\n",
    "# Store results\n",
    "results = []\n",
    "\n",
    "# Iterate over each dataset in the folder\n",
    "for filename in os.listdir(datasets_folder_path):\n",
    "    if filename.endswith('.csv'):\n",
    "        dataset_path = os.path.join(datasets_folder_path, filename)\n",
    "        data = pd.read_csv(dataset_path)\n",
    "        \n",
    "        print(f\"Processing {dataset_path}...\")\n",
    "        \n",
    "        # Merge with truth_data on date and location\n",
    "        merged_data = pd.merge(truth_data, data, on=['date', 'location_key'])\n",
    "        \n",
    "        for column in data.columns:\n",
    "            if column not in ['date', 'location_key']:\n",
    "                # Drop rows where the current column or truth_value has NaN\n",
    "                clean_merged_data = merged_data.dropna(subset=[column, 'truth_value'])\n",
    "\n",
    "                # Check if there's still data left after dropping NaNs\n",
    "                if not clean_merged_data.empty:\n",
    "                    # Check if the data is numeric\n",
    "                    if pd.api.types.is_numeric_dtype(clean_merged_data[column]):\n",
    "                        # Prepare data for regression\n",
    "                        X = sm.add_constant(clean_merged_data[column])  # Add constant\n",
    "                        y = clean_merged_data['truth_value']\n",
    "\n",
    "                        # Run linear regression\n",
    "                        model = sm.OLS(y, X).fit()\n",
    "\n",
    "                        # Record the result\n",
    "                        results.append({\n",
    "                            'filename': filename,\n",
    "                            'variable': column,\n",
    "                            'correlation': model.rsquared\n",
    "                        })\n",
    "\n",
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Sort results by correlation in descending order\n",
    "results_df = results_df.sort_values(by='correlation', ascending=False)\n",
    "\n",
    "results_df.to_csv('processed_data/OLS_correlation_ranking.csv', index=False)\n",
    "\n",
    "# Output the sorted results\n",
    "results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ranked data merging main dataset\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Load the truth data\n",
    "truth_data_path = 'processed_data/unique_truth_main_dataset.csv'\n",
    "datasets_folder_path = 'raw_datasets/supplementary_dataset_TS'\n",
    "truth_data = pd.read_csv(truth_data_path)\n",
    "truth_data.rename(columns={'abbreviation': 'location_key', 'target_end_date': 'date'}, inplace=True)\n",
    "truth_data['location_key'] = 'US_' + truth_data['location_key']\n",
    "\n",
    "# Load the correlation data\n",
    "correlation_data_path = 'processed_data\\OLS_correlation_ranking.csv'\n",
    "correlation_data = pd.read_csv(correlation_data_path)\n",
    "\n",
    "# Filter variables with R^2 greater than 0.04\n",
    "selected_variables = correlation_data[correlation_data['correlation'] > 0.04]\n",
    "\n",
    "# Iterate over each dataset in the folder\n",
    "for filename in os.listdir(datasets_folder_path):\n",
    "    if filename.endswith('.csv'):\n",
    "        dataset_path = os.path.join(datasets_folder_path, filename)\n",
    "        data = pd.read_csv(dataset_path)\n",
    "        \n",
    "        print(f\"Processing {dataset_path}...\")\n",
    "        \n",
    "        # Process only selected variables for the current dataset\n",
    "        for _, row in selected_variables.iterrows():\n",
    "            if row['filename'] == filename:\n",
    "                column = row['variable']\n",
    "                \n",
    "                # Verify if the column exists in the current dataset\n",
    "                if column in data.columns:\n",
    "                    # Create a temporary DataFrame to hold the current column's data\n",
    "                    temp_data = data[['date', 'location_key', column]].dropna()\n",
    "\n",
    "                    # Merge the temporary DataFrame with the truth_data DataFrame\n",
    "                    truth_data = pd.merge(truth_data, temp_data, on=['date', 'location_key'], how='left', suffixes=('', '_temp'))\n",
    "\n",
    "                    # Check if the merged column exists (avoid overwriting existing data)\n",
    "                    if f'{column}_temp' in truth_data.columns:\n",
    "                        # If the column already exists in truth_data, we combine the original and new columns\n",
    "                        truth_data[column] = truth_data[column].combine_first(truth_data[f'{column}_temp'])\n",
    "                        # Drop the temporary column after combining\n",
    "                        truth_data.drop(columns=[f'{column}_temp'], inplace=True)\n",
    "\n",
    "# After processing all files, save the updated truth_data DataFrame\n",
    "truth_data.to_csv('processed_data/ranked_merged_dataset_main.csv', index=False)\n",
    "\n",
    "truth_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "results_df = pd.read_csv('processed_data/ranked_merged_dataset_main.csv')\n",
    "\n",
    "missing_value_percentages = results_df.select_dtypes(include=[np.number]).isnull().mean().sort_values() * 100\n",
    "\n",
    "print(missing_value_percentages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main dataset\n",
    "\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the dataset\n",
    "final_dataset = pd.read_csv('processed_data/ranked_merged_dataset.csv')\n",
    "final_dataset.drop(columns=['location_name'], inplace=True)\n",
    "\n",
    "# Load correlation data to filter variables\n",
    "selected_variables = [\n",
    "    'cumulative_deceased',\n",
    "    'new_confirmed',\n",
    "    'cumulative_confirmed',\n",
    "    'new_hospitalized_patients',\n",
    "    'current_hospitalized_patients',\n",
    "    'cumulative_hospitalized_patients',\n",
    "    'current_intensive_care_patients',\n",
    "    'cumulative_vaccine_doses_administered',\n",
    "    'cumulative_persons_vaccinated',\n",
    "    'cumulative_persons_fully_vaccinated',\n",
    "    'new_tested',\n",
    "    'cumulative_tested',\n",
    "    'cumulative_recovered',\n",
    "    'new_recovered',\n",
    "    'location_key_US_AK', 'location_key_US_AL', 'location_key_US_AR', 'location_key_US_AZ', 'location_key_US_CA', 'location_key_US_CO', 'location_key_US_CT', 'location_key_US_DE', 'location_key_US_FL', 'location_key_US_GA', 'location_key_US_HI', 'location_key_US_IA', 'location_key_US_ID', 'location_key_US_IL', 'location_key_US_IN', 'location_key_US_KS', 'location_key_US_KY', 'location_key_US_LA', 'location_key_US_MA', 'location_key_US_MD', 'location_key_US_ME', 'location_key_US_MI', 'location_key_US_MN', 'location_key_US_MO', 'location_key_US_MS', 'location_key_US_MT', 'location_key_US_NC', 'location_key_US_ND', 'location_key_US_NE', 'location_key_US_NH', 'location_key_US_NJ', 'location_key_US_NM', 'location_key_US_NV', 'location_key_US_NY', 'location_key_US_OH', 'location_key_US_OK', 'location_key_US_OR', 'location_key_US_PA', 'location_key_US_PR', 'location_key_US_RI', 'location_key_US_SC', 'location_key_US_SD', 'location_key_US_TN', 'location_key_US_TX', 'location_key_US_UT', 'location_key_US_VA', 'location_key_US_VT', 'location_key_US_WA', 'location_key_US_WI', 'location_key_US_WV', 'location_key_US_WY'\n",
    "]\n",
    "\n",
    "pre_selected_variables = [\n",
    "    'cumulative_deceased',\n",
    "    'new_confirmed',\n",
    "    'cumulative_confirmed',\n",
    "    'new_hospitalized_patients',\n",
    "    'current_hospitalized_patients',\n",
    "    'cumulative_hospitalized_patients',\n",
    "    'current_intensive_care_patients',\n",
    "    'cumulative_vaccine_doses_administered',\n",
    "    'cumulative_persons_vaccinated',\n",
    "    'cumulative_persons_fully_vaccinated',\n",
    "    'new_tested',\n",
    "    'cumulative_tested',\n",
    "    'cumulative_recovered',\n",
    "    'new_recovered'\n",
    "]\n",
    "\n",
    "# Impute missing values for numerical columns\n",
    "for column in pre_selected_variables:\n",
    "    if final_dataset[column].dtype != 'object':  # If column is numerical\n",
    "        final_dataset[column].fillna(final_dataset[column].mean(), inplace=True)\n",
    "        \n",
    "X = X.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "X.fillna(X.mean(), inplace=True)\n",
    "\n",
    "y = pd.to_numeric(y, errors='coerce').fillna(y.mean())\n",
    "\n",
    "# Create dummy variables for 'location_key'\n",
    "final_dataset = pd.get_dummies(final_dataset, columns=['location_key'], dtype=int)\n",
    "\n",
    "# Define independent variables (X) and dependent variable (y)\n",
    "# Ensure 'location_key' dummies are included in X\n",
    "X = final_dataset[[col for col in final_dataset.columns if col in selected_variables or 'location_key_' in col]]\n",
    "y = final_dataset['truth_value']\n",
    "\n",
    "# Add a constant to the model (intercept)\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Use sm.OLS to perform the regression and fit the model\n",
    "model = sm.OLS(y, X).fit()\n",
    "\n",
    "# Print the summary of the regression\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Adjust these paths to where your datasets are located\n",
    "truth_data_path = 'processed_data/unique_truth_google_dataset.csv'\n",
    "datasets_folder_path = 'raw_datasets/supplementary_dataset_TS'\n",
    "\n",
    "# Load the truth data\n",
    "truth_data = pd.read_csv(truth_data_path)\n",
    "\n",
    "# Ensure 'new_confirmed' is in truth_data\n",
    "if 'new_confirmed' not in truth_data.columns:\n",
    "    raise ValueError(\"Column 'new_confirmed' not found in truth data.\")\n",
    "\n",
    "results = []\n",
    "\n",
    "# Iterate over each dataset in the folder\n",
    "for filename in os.listdir(datasets_folder_path):\n",
    "    if filename.endswith('.csv'):\n",
    "        dataset_path = os.path.join(datasets_folder_path, filename)\n",
    "        data = pd.read_csv(dataset_path)\n",
    "\n",
    "        # Debug: Print columns to verify structure\n",
    "        print(f\"Processing {filename}, columns: {data.columns.tolist()}\")\n",
    "\n",
    "        # Merge with truth_data on date and location\n",
    "        merged_data = pd.merge(truth_data, data, on=['date', 'location_key'])\n",
    "\n",
    "        # Ensure 'new_confirmed' is in merged_data\n",
    "        if 'new_confirmed' not in merged_data.columns:\n",
    "            print(f\"Warning: 'new_confirmed' not found after merging with {filename}. Skipping this file.\")\n",
    "            continue  # Skip this file if 'new_confirmed' is not in the merged data\n",
    "\n",
    "        for column in data.columns:\n",
    "            if column not in ['date', 'location_key']:\n",
    "                try:\n",
    "                    # Drop rows where the current column or truth_value has NaN\n",
    "                    clean_merged_data = merged_data.dropna(subset=[column, 'new_confirmed'])\n",
    "\n",
    "                    if not clean_merged_data.empty:\n",
    "                        if pd.api.types.is_numeric_dtype(clean_merged_data[column]):\n",
    "                            X = sm.add_constant(clean_merged_data[column])  # Add constant\n",
    "                            y = clean_merged_data['new_confirmed']\n",
    "\n",
    "                            model = sm.OLS(y, X).fit()\n",
    "\n",
    "                            results.append({\n",
    "                                'filename': filename,\n",
    "                                'variable': column,\n",
    "                                'correlation': model.rsquared\n",
    "                            })\n",
    "                except KeyError as e:\n",
    "                    print(f\"KeyError encountered for column {e} in file {filename}. It might be missing after the merge.\")\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.sort_values(by='correlation', ascending=False)\n",
    "results_df.to_csv('processed_data/Google_OLS_correlation_ranking.csv', index=False)\n",
    "print(results_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ranked data merging main dataset\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Load the truth data\n",
    "truth_data_path = 'processed_data/unique_truth_google_dataset.csv'\n",
    "datasets_folder_path = 'raw_datasets/supplementary_dataset_TS'\n",
    "truth_data = pd.read_csv(truth_data_path)\n",
    "\n",
    "# Load the correlation data\n",
    "correlation_data_path = 'processed_data/Google_OLS_correlation_ranking.csv'\n",
    "correlation_data = pd.read_csv(correlation_data_path)\n",
    "\n",
    "# Filter variables with R^2 greater than 0.04\n",
    "selected_variables = correlation_data[correlation_data['correlation'] > 0.2]\n",
    "\n",
    "# Iterate over each dataset in the folder\n",
    "for filename in os.listdir(datasets_folder_path):\n",
    "    if filename.endswith('.csv'):\n",
    "        dataset_path = os.path.join(datasets_folder_path, filename)\n",
    "        data = pd.read_csv(dataset_path)\n",
    "        \n",
    "        print(f\"Processing {dataset_path}...\")\n",
    "        \n",
    "        # Process only selected variables for the current dataset\n",
    "        for _, row in selected_variables.iterrows():\n",
    "            if row['filename'] == filename:\n",
    "                column = row['variable']\n",
    "                \n",
    "                # Verify if the column exists in the current dataset\n",
    "                if column in data.columns:\n",
    "                    # Create a temporary DataFrame to hold the current column's data\n",
    "                    temp_data = data[['date', 'location_key', column]].dropna()\n",
    "\n",
    "                    # Merge the temporary DataFrame with the truth_data DataFrame\n",
    "                    truth_data = pd.merge(truth_data, temp_data, on=['date', 'location_key'], how='left', suffixes=('', '_temp'))\n",
    "\n",
    "                    # Check if the merged column exists (avoid overwriting existing data)\n",
    "                    if f'{column}_temp' in truth_data.columns:\n",
    "                        # If the column already exists in truth_data, we combine the original and new columns\n",
    "                        truth_data[column] = truth_data[column].combine_first(truth_data[f'{column}_temp'])\n",
    "                        # Drop the temporary column after combining\n",
    "                        truth_data.drop(columns=[f'{column}_temp'], inplace=True)\n",
    "\n",
    "# After processing all files, save the updated truth_data DataFrame\n",
    "truth_data.to_csv('processed_data/Google_ranked_merged_dataset.csv', index=False)\n",
    "\n",
    "truth_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_confirmed                                           0.000000\n",
      "new_hospitalized_patients                               6.508908\n",
      "current_hospitalized_patients                          12.399941\n",
      "current_intensive_care_patients                        16.455790\n",
      "current_ventilator_patients                            83.167952\n",
      "new_intensive_care_patients                            92.434247\n",
      "cumulative_intensive_care_patients                     93.011546\n",
      "new_confirmed_age_2                                    94.332141\n",
      "new_confirmed_age_0                                    94.332141\n",
      "new_confirmed_age_3                                    94.332141\n",
      "new_confirmed_age_1                                    94.332141\n",
      "new_deceased_age_9                                     94.525803\n",
      "lawatlas_home_except_obtaining_necessary_supplies      96.021616\n",
      "lawatlas_home_except_engaging_in_outdoor_activities    96.021616\n",
      "new_confirmed_age_6                                    96.775978\n",
      "new_confirmed_age_7                                    96.875576\n",
      "new_confirmed_age_8                                    97.006529\n",
      "new_confirmed_age_9                                    97.006529\n",
      "new_confirmed_female                                   97.428898\n",
      "new_confirmed_male                                     97.428898\n",
      "new_tested_age_2                                       98.819580\n",
      "new_tested_age_5                                       98.819580\n",
      "new_tested_female                                      98.819580\n",
      "new_tested_age_1                                       98.819580\n",
      "new_tested_male                                        98.819580\n",
      "new_tested_age_0                                       98.819580\n",
      "new_tested_age_4                                       98.819580\n",
      "new_tested_age_3                                       98.819580\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "results_df = pd.read_csv('processed_data/Google_ranked_merged_dataset.csv')\n",
    "\n",
    "missing_value_percentages = results_df.select_dtypes(include=[np.number]).isnull().mean().sort_values() * 100\n",
    "\n",
    "print(missing_value_percentages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:          new_confirmed   R-squared:                       0.476\n",
      "Model:                            OLS   Adj. R-squared:                  0.475\n",
      "Method:                 Least Squares   F-statistic:                     847.2\n",
      "Date:                Tue, 05 Mar 2024   Prob (F-statistic):               0.00\n",
      "Time:                        12:22:47   Log-Likelihood:            -5.2593e+05\n",
      "No. Observations:               54218   AIC:                         1.052e+06\n",
      "Df Residuals:                   54159   BIC:                         1.053e+06\n",
      "Df Model:                          58                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "===================================================================================================\n",
      "                                      coef    std err          t      P>|t|      [0.025      0.975]\n",
      "---------------------------------------------------------------------------------------------------\n",
      "const                             -52.3825     21.376     -2.451      0.014     -94.280     -10.485\n",
      "new_hospitalized_patients           6.5365      0.148     44.243      0.000       6.247       6.826\n",
      "current_hospitalized_patients       4.4452      0.048     93.387      0.000       4.352       4.538\n",
      "current_intensive_care_patients   -13.6321      0.172    -79.117      0.000     -13.970     -13.294\n",
      "location_key_US_AK                150.5648    126.384      1.191      0.234     -97.149     398.279\n",
      "location_key_US_AL                241.0849    126.099      1.912      0.056      -6.071     488.241\n",
      "location_key_US_AR                833.0760    126.392      6.591      0.000     585.346    1080.806\n",
      "location_key_US_AS               -756.3469    126.163     -5.995      0.000   -1003.628    -509.066\n",
      "location_key_US_AZ                 20.8398    125.984      0.165      0.869    -226.091     267.770\n",
      "location_key_US_CA               1613.5686    137.516     11.734      0.000    1344.036    1883.101\n",
      "location_key_US_CO               1026.6268    126.210      8.134      0.000     779.255    1273.998\n",
      "location_key_US_CT               -265.7145    126.075     -2.108      0.035    -512.823     -18.606\n",
      "location_key_US_DC               -264.1868    126.241     -2.093      0.036    -511.619     -16.754\n",
      "location_key_US_DE                -19.7977    126.281     -0.157      0.875    -267.310     227.715\n",
      "location_key_US_FL              -2641.5133    133.846    -19.735      0.000   -2903.853   -2379.173\n",
      "location_key_US_GA               -726.8305    126.752     -5.734      0.000    -975.266    -478.395\n",
      "location_key_US_GU               -941.9787    125.935     -7.480      0.000   -1188.812    -695.146\n",
      "location_key_US_HI               -140.2051    126.296     -1.110      0.267    -387.747     107.337\n",
      "location_key_US_IA                -78.5719    126.124     -0.623      0.533    -325.775     168.631\n",
      "location_key_US_ID                295.1263    126.212      2.338      0.019      47.751     542.502\n",
      "location_key_US_IL                341.4097    126.521      2.698      0.007      93.427     589.392\n",
      "location_key_US_IN                459.3275    126.188      3.640      0.000     211.997     706.658\n",
      "location_key_US_KS                 86.2520    126.009      0.684      0.494    -160.726     333.230\n",
      "location_key_US_KY               -158.0464    126.230     -1.252      0.211    -405.458      89.365\n",
      "location_key_US_LA               -245.9200    125.882     -1.954      0.051    -492.651       0.811\n",
      "location_key_US_MA                383.5733    125.920      3.046      0.002     136.769     630.378\n",
      "location_key_US_MD               -362.8998    125.887     -2.883      0.004    -609.639    -116.161\n",
      "location_key_US_ME                 75.9950    126.300      0.602      0.547    -171.553     323.543\n",
      "location_key_US_MI                 31.1432    126.141      0.247      0.805    -216.094     278.381\n",
      "location_key_US_MN                426.6254    125.968      3.387      0.001     179.727     673.524\n",
      "location_key_US_MO                 72.4418    126.029      0.575      0.565    -174.576     319.459\n",
      "location_key_US_MP              -1470.4156    126.015    -11.669      0.000   -1717.406   -1223.425\n",
      "location_key_US_MS                136.1383    125.978      1.081      0.280    -110.779     383.056\n",
      "location_key_US_MT                177.1628    126.269      1.403      0.161     -70.326     424.652\n",
      "location_key_US_NC                835.8621    126.056      6.631      0.000     588.791    1082.933\n",
      "location_key_US_ND                299.9037    126.308      2.374      0.018      52.338     547.469\n",
      "location_key_US_NE                -62.5433    126.164     -0.496      0.620    -309.826     184.740\n",
      "location_key_US_NH                289.3753    126.296      2.291      0.022      41.835     536.916\n",
      "location_key_US_NJ              -1075.3676    126.744     -8.485      0.000   -1323.786    -826.949\n",
      "location_key_US_NM                438.5136    126.181      3.475      0.001     191.199     685.828\n",
      "location_key_US_NV               -384.5774    125.967     -3.053      0.002    -631.475    -137.680\n",
      "location_key_US_NY              -1516.9089    131.167    -11.565      0.000   -1773.996   -1259.821\n",
      "location_key_US_OH               -334.4314    126.616     -2.641      0.008    -582.600     -86.263\n",
      "location_key_US_OK                 45.3159    126.109      0.359      0.719    -201.859     292.491\n",
      "location_key_US_OR                 59.6543    126.148      0.473      0.636    -187.597     306.906\n",
      "location_key_US_PA              -1491.4880    127.279    -11.718      0.000   -1740.956   -1242.020\n",
      "location_key_US_PR                344.4830    126.268      2.728      0.006      96.996     591.970\n",
      "location_key_US_RI                 12.8870    126.415      0.102      0.919    -234.887     260.661\n",
      "location_key_US_SC                171.9566    125.883      1.366      0.172     -74.774     418.687\n",
      "location_key_US_SD                159.8726    126.269      1.266      0.205     -87.616     407.361\n",
      "location_key_US_TN                634.3288    126.070      5.032      0.000     387.230     881.428\n",
      "location_key_US_TX               1083.3252    143.810      7.533      0.000     801.457    1365.193\n",
      "location_key_US_UT                832.0863    126.173      6.595      0.000     584.786    1079.387\n",
      "location_key_US_VA                 64.1854    125.912      0.510      0.610    -182.604     310.974\n",
      "location_key_US_VI               -202.4867    126.388     -1.602      0.109    -450.208      45.235\n",
      "location_key_US_VT                339.5800    126.410      2.686      0.007      91.816     587.344\n",
      "location_key_US_WA                570.1521    125.378      4.547      0.000     324.409     815.895\n",
      "location_key_US_WI                328.0291    126.005      2.603      0.009      81.059     574.999\n",
      "location_key_US_WV                -31.4473    126.085     -0.249      0.803    -278.575     215.680\n",
      "location_key_US_WY                238.8276    126.346      1.890      0.059      -8.812     486.467\n",
      "==============================================================================\n",
      "Omnibus:                    98051.291   Durbin-Watson:                   1.663\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):        423937062.510\n",
      "Skew:                          12.807   Prob(JB):                         0.00\n",
      "Kurtosis:                     435.438   Cond. No.                     5.92e+17\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The smallest eigenvalue is 5.61e-25. This might indicate that there are\n",
      "strong multicollinearity problems or that the design matrix is singular.\n"
     ]
    }
   ],
   "source": [
    "# Google dataset\n",
    "\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the dataset\n",
    "final_dataset = pd.read_csv('processed_data/Google_ranked_merged_dataset.csv')\n",
    "\n",
    "# Load correlation data to filter variables\n",
    "selected_variables = [\n",
    "    'new_hospitalized_patients',\n",
    "    'current_hospitalized_patients',\n",
    "    'current_intensive_care_patients',\n",
    "    'location_key_US_AK', 'location_key_US_AL', 'location_key_US_AR', 'location_key_US_AZ', 'location_key_US_CA', 'location_key_US_CO', 'location_key_US_CT', 'location_key_US_DE', 'location_key_US_FL', 'location_key_US_GA', 'location_key_US_HI', 'location_key_US_IA', 'location_key_US_ID', 'location_key_US_IL', 'location_key_US_IN', 'location_key_US_KS', 'location_key_US_KY', 'location_key_US_LA', 'location_key_US_MA', 'location_key_US_MD', 'location_key_US_ME', 'location_key_US_MI', 'location_key_US_MN', 'location_key_US_MO', 'location_key_US_MS', 'location_key_US_MT', 'location_key_US_NC', 'location_key_US_ND', 'location_key_US_NE', 'location_key_US_NH', 'location_key_US_NJ', 'location_key_US_NM', 'location_key_US_NV', 'location_key_US_NY', 'location_key_US_OH', 'location_key_US_OK', 'location_key_US_OR', 'location_key_US_PA', 'location_key_US_PR', 'location_key_US_RI', 'location_key_US_SC', 'location_key_US_SD', 'location_key_US_TN', 'location_key_US_TX', 'location_key_US_UT', 'location_key_US_VA', 'location_key_US_VT', 'location_key_US_WA', 'location_key_US_WI', 'location_key_US_WV', 'location_key_US_WY'\n",
    "]\n",
    "\n",
    "pre_selected_variables = [\n",
    "    'new_hospitalized_patients',\n",
    "    'current_hospitalized_patients',\n",
    "    'current_intensive_care_patients'\n",
    "]\n",
    "\n",
    "for column in pre_selected_variables:\n",
    "    if final_dataset[column].dtype != 'object':  # If column is numerical\n",
    "        final_dataset[column].fillna(final_dataset[column].mean(), inplace=True)\n",
    "\n",
    "# Assuming 'location_key' needs to be converted into dummies and included in the regression\n",
    "final_dataset = pd.get_dummies(final_dataset, columns=['location_key'], dtype=int)\n",
    "\n",
    "# Now define X and y\n",
    "selected_variables = pre_selected_variables + [col for col in final_dataset.columns if 'location_key_' in col]\n",
    "\n",
    "X = final_dataset[selected_variables]  # Ensure this includes all your independent variables\n",
    "y = final_dataset['new_confirmed'].apply(pd.to_numeric, errors='coerce')  # Adjust column name as needed\n",
    "\n",
    "# Add constant to X\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Handle any potential NaN values in X or y before regression\n",
    "X.fillna(X.mean(), inplace=True)\n",
    "y.fillna(y.mean(), inplace=True)\n",
    "\n",
    "# Perform the regression\n",
    "model = sm.OLS(y, X).fit()\n",
    "\n",
    "# Print the summary\n",
    "print(model.summary())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
